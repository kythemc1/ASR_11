{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-05-17T00:48:36.795491Z",
     "iopub.status.busy": "2025-05-17T00:48:36.795267Z",
     "iopub.status.idle": "2025-05-17T00:50:19.54394Z",
     "shell.execute_reply": "2025-05-17T00:50:19.542937Z",
     "shell.execute_reply.started": "2025-05-17T00:48:36.795475Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install --upgrade datasets[audio] transformers accelerate evaluate jiwer tensorboard gradio\n",
    "!pip install pyannote.audio\n",
    "!pip install jiwer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-17T00:52:43.504785Z",
     "iopub.status.busy": "2025-05-17T00:52:43.503916Z",
     "iopub.status.idle": "2025-05-17T00:52:50.890554Z",
     "shell.execute_reply": "2025-05-17T00:52:50.889887Z",
     "shell.execute_reply.started": "2025-05-17T00:52:43.504757Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Install cpu/cuda pytorch (>=1.9) dependency from pytorch.org, e.g.:\n",
    "!pip install torch torchaudio -f https://download.pytorch.org/whl/cpu/torch_stable.html\n",
    "!pip install deepfilternet\n",
    "!pip install deepfilternet[train]\n",
    "\n",
    "!pip install -U bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-17T00:52:59.873174Z",
     "iopub.status.busy": "2025-05-17T00:52:59.872421Z",
     "iopub.status.idle": "2025-05-17T00:53:05.273705Z",
     "shell.execute_reply": "2025-05-17T00:53:05.272974Z",
     "shell.execute_reply.started": "2025-05-17T00:52:59.873148Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torchaudio\n",
    "\n",
    "\n",
    "data_dir = '/kaggle/input/data-100h/vlsp2020_train_set_02' \n",
    "\n",
    "MAX_SAMPLES = 10000\n",
    "TRAIN_SIZE = 0.7\n",
    "VAL_SIZE = 0.2\n",
    "TEST_SIZE = 0.1\n",
    "rows = []\n",
    "\n",
    "from df.enhance import enhance, init_df, load_audio\n",
    "import torch\n",
    "\n",
    "TARGET_SR = 16000\n",
    "\n",
    "df_model, df_state, _ = init_df()\n",
    "resampler = torchaudio.transforms.Resample(orig_freq=df_state.sr(), new_freq=TARGET_SR)\n",
    "\n",
    "def enhance_waveform(noisy_path):\n",
    "    audio, sr = load_audio(noisy_path, sr=df_state.sr())\n",
    "    enhanced = enhance(df_model, df_state, audio)\n",
    "\n",
    "    resampled = resampler(enhanced)\n",
    "    return resampled, TARGET_SR\n",
    "\n",
    "df_state.sr()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-17T00:53:09.694479Z",
     "iopub.status.busy": "2025-05-17T00:53:09.693645Z",
     "iopub.status.idle": "2025-05-17T00:53:10.296736Z",
     "shell.execute_reply": "2025-05-17T00:53:10.296193Z",
     "shell.execute_reply.started": "2025-05-17T00:53:09.694453Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login(token=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-17T00:53:13.014182Z",
     "iopub.status.busy": "2025-05-17T00:53:13.0136Z",
     "iopub.status.idle": "2025-05-17T00:53:13.813609Z",
     "shell.execute_reply": "2025-05-17T00:53:13.812933Z",
     "shell.execute_reply.started": "2025-05-17T00:53:13.01416Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import WhisperFeatureExtractor, WhisperTokenizer, WhisperProcessor\n",
    "\n",
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(\"openai/whisper-medium\")\n",
    "\n",
    "tokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-medium\", language=\"vietnamese\", task=\"transcribe\")\n",
    "\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-medium\", language=\"vietnamese\", task=\"transcribe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-17T00:53:34.447246Z",
     "iopub.status.busy": "2025-05-17T00:53:34.446528Z",
     "iopub.status.idle": "2025-05-17T00:53:50.061767Z",
     "shell.execute_reply": "2025-05-17T00:53:50.060988Z",
     "shell.execute_reply.started": "2025-05-17T00:53:34.447216Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"wer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-17T01:29:38.45442Z",
     "iopub.status.busy": "2025-05-17T01:29:38.453862Z",
     "iopub.status.idle": "2025-05-17T01:29:40.545473Z",
     "shell.execute_reply": "2025-05-17T01:29:40.544938Z",
     "shell.execute_reply.started": "2025-05-17T01:29:38.454398Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import WhisperForConditionalGeneration\n",
    "\n",
    "# Khởi tạo base model\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-medium\")\n",
    "# model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-medium\", load_in_8bit=True, device_map=\"auto\")\n",
    "\n",
    "# Cấu hình generation\n",
    "model.config.forced_decoder_ids = None\n",
    "model.config.suppress_tokens = []\n",
    "\n",
    "model.generation_config.language = \"vietnamese\"\n",
    "model.generation_config.task = \"transcribe\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-17T00:54:14.22544Z",
     "iopub.status.busy": "2025-05-17T00:54:14.225049Z",
     "iopub.status.idle": "2025-05-17T00:54:14.411725Z",
     "shell.execute_reply": "2025-05-17T00:54:14.410956Z",
     "shell.execute_reply.started": "2025-05-17T00:54:14.225412Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from peft import LoraConfig , LoraConfig, get_peft_model\n",
    "# Cấu hình LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=8,  # rank của LoRA, có thể tăng lên nếu GPU đủ mạnh\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  # các module attention\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    # task_type=TaskType.SEQ_2_SEQ_LM\n",
    ")\n",
    "\n",
    "# Áp dụng LoRA vào model\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-17T00:54:18.816731Z",
     "iopub.status.busy": "2025-05-17T00:54:18.816099Z",
     "iopub.status.idle": "2025-05-17T00:54:22.369645Z",
     "shell.execute_reply": "2025-05-17T00:54:22.36895Z",
     "shell.execute_reply.started": "2025-05-17T00:54:18.816699Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_audio_files():\n",
    "    \"\"\"Get list of audio and transcript files\"\"\"\n",
    "    audio_files = []\n",
    "    transcript_files = []\n",
    "\n",
    "    for root, dirs, files in os.walk(data_dir):\n",
    "        for file in files:\n",
    "            if file.endswith('.wav'):\n",
    "                audio_files.append(os.path.join(root, file))\n",
    "                transcript_files.append(os.path.join(root, file.replace('.wav', '.txt')))\n",
    "\n",
    "    # Sort files to ensure consistent ordering\n",
    "    audio_files.sort()\n",
    "    transcript_files.sort()\n",
    "\n",
    "    # Limit to MAX_SAMPLES\n",
    "    audio_files = audio_files[:MAX_SAMPLES]\n",
    "    transcript_files = transcript_files[:MAX_SAMPLES]\n",
    "\n",
    "    return audio_files, transcript_files\n",
    "\n",
    "def load_sample(audio_path, transcript_path):\n",
    "    \"\"\"Load audio and transcript for a single sample\"\"\"\n",
    "    try:\n",
    "        # Load audio\n",
    "        waveform, sr = enhance_waveform(audio_path)\n",
    "\n",
    "        # Load transcript\n",
    "        with open(transcript_path, 'r', encoding='utf-8') as f:\n",
    "            transcript = f.read().strip()\n",
    "\n",
    "        return {\n",
    "            'audio': waveform.numpy(),\n",
    "            'transcript': transcript,\n",
    "            'sample_rate': sr,\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {audio_path}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def create_dataset():\n",
    "    \"\"\"Create dataset splits from audio files\"\"\"\n",
    "    # Initialize dataset dictionary\n",
    "    vlsp_dict = DatasetDict()\n",
    "\n",
    "    # Get audio files\n",
    "    audio_files, transcript_files = get_audio_files()\n",
    "\n",
    "    # Process files and create splits\n",
    "    train_data = []\n",
    "    val_data = []\n",
    "    test_data = []\n",
    "\n",
    "    # Process files with progress bar\n",
    "    for i in tqdm.tqdm(range(len(audio_files))):\n",
    "        sample = load_sample(audio_files[i], transcript_files[i])\n",
    "        if sample is not None:\n",
    "            if i < TRAIN_SIZE * MAX_SAMPLES:\n",
    "                train_data.append(sample)\n",
    "            elif i < (TRAIN_SIZE + VAL_SIZE) * MAX_SAMPLES:\n",
    "                val_data.append(sample)\n",
    "            else:\n",
    "                test_data.append(sample)\n",
    "\n",
    "    # Create dataset splits\n",
    "    chunk_size = 1000\n",
    "    chunks = [Dataset.from_list(train_data[i:i+chunk_size]) for i in range(0, len(train_data), chunk_size)]\n",
    "\n",
    "    # Concatenate all chunks into one Dataset\n",
    "    vlsp_dict[\"train\"] = concatenate_datasets(chunks)\n",
    "\n",
    "    vlsp_dict[\"val\"] = Dataset.from_list(val_data)\n",
    "    vlsp_dict[\"test\"] = Dataset.from_list(test_data)\n",
    "\n",
    "    # Print dataset statistics\n",
    "    print(f\"Train set size: {len(vlsp_dict['train'])}\")\n",
    "    print(f\"Validation set size: {len(vlsp_dict['val'])}\")\n",
    "    print(f\"Test set size: {len(vlsp_dict['test'])}\")\n",
    "\n",
    "    return vlsp_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-17T00:54:27.920485Z",
     "iopub.status.busy": "2025-05-17T00:54:27.920215Z",
     "iopub.status.idle": "2025-05-17T01:15:52.220349Z",
     "shell.execute_reply": "2025-05-17T01:15:52.219656Z",
     "shell.execute_reply.started": "2025-05-17T00:54:27.920465Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "from datasets import Dataset, DatasetDict, concatenate_datasets\n",
    "import tqdm as tqdm\n",
    "from IPython.display import Audio\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "vlsp_dict = create_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-17T01:15:57.723697Z",
     "iopub.status.busy": "2025-05-17T01:15:57.723421Z",
     "iopub.status.idle": "2025-05-17T01:15:57.761077Z",
     "shell.execute_reply": "2025-05-17T01:15:57.760511Z",
     "shell.execute_reply.started": "2025-05-17T01:15:57.723666Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "vlsp_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-17T01:16:01.041186Z",
     "iopub.status.busy": "2025-05-17T01:16:01.040906Z",
     "iopub.status.idle": "2025-05-17T01:27:18.991326Z",
     "shell.execute_reply": "2025-05-17T01:27:18.990172Z",
     "shell.execute_reply.started": "2025-05-17T01:16:01.041166Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Audio\n",
    "\n",
    "\n",
    "\n",
    "def prepare_dataset(batch):\n",
    "    # load and resample audio data from 48 to 16kHz\n",
    "    audio = batch[\"audio\"]\n",
    "\n",
    "    # compute log-Mel input features from input audio array\n",
    "    batch[\"input_features\"] = feature_extractor(audio[0], sampling_rate=batch[\"sample_rate\"]).input_features[0]\n",
    "\n",
    "    # encode target text to label ids\n",
    "    batch[\"labels\"] = tokenizer(batch[\"transcript\"], truncation=True, max_length=model.max_target_positions).input_ids\n",
    "    return batch\n",
    "\n",
    "\n",
    "\n",
    "vlsp_dict = vlsp_dict.map(prepare_dataset, remove_columns=vlsp_dict.column_names[\"train\"], num_proc=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-17T01:29:47.767131Z",
     "iopub.status.busy": "2025-05-17T01:29:47.766412Z",
     "iopub.status.idle": "2025-05-17T01:29:47.772234Z",
     "shell.execute_reply": "2025-05-17T01:29:47.771435Z",
     "shell.execute_reply.started": "2025-05-17T01:29:47.767107Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "vlsp_dict[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-17T01:29:50.390331Z",
     "iopub.status.busy": "2025-05-17T01:29:50.389559Z",
     "iopub.status.idle": "2025-05-17T01:29:50.400316Z",
     "shell.execute_reply": "2025-05-17T01:29:50.399592Z",
     "shell.execute_reply.started": "2025-05-17T01:29:50.390303Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "import torch\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "    decoder_start_token_id: int\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # Pad input features\n",
    "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "\n",
    "        # Pad labels\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "\n",
    "        # Replace padding with -100\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(\n",
    "            labels_batch.attention_mask.ne(1), -100\n",
    "        )\n",
    "\n",
    "        # Remove bos token if present\n",
    "        if (labels[:, 0] == self.decoder_start_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "\n",
    "        batch = {\n",
    "            \"input_features\": batch[\"input_features\"],\n",
    "            \"labels\": labels,\n",
    "        }\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-17T01:29:54.337154Z",
     "iopub.status.busy": "2025-05-17T01:29:54.33643Z",
     "iopub.status.idle": "2025-05-17T01:29:54.340416Z",
     "shell.execute_reply": "2025-05-17T01:29:54.339738Z",
     "shell.execute_reply.started": "2025-05-17T01:29:54.337129Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(\n",
    "    processor=processor,\n",
    "    decoder_start_token_id=model.config.decoder_start_token_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-17T01:29:58.34734Z",
     "iopub.status.busy": "2025-05-17T01:29:58.346595Z",
     "iopub.status.idle": "2025-05-17T01:29:58.3516Z",
     "shell.execute_reply": "2025-05-17T01:29:58.350874Z",
     "shell.execute_reply.started": "2025-05-17T01:29:58.347317Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    pred_ids = pred.predictions\n",
    "    label_ids = pred.label_ids\n",
    "\n",
    "    # replace -100 with the pad_token_id\n",
    "    label_ids[label_ids == -100] = tokenizer.pad_token_id\n",
    "\n",
    "    # we do not want to group tokens when computing the metrics\n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "\n",
    "    wer = 100 * metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return {\"wer\": wer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-17T01:30:01.798973Z",
     "iopub.status.busy": "2025-05-17T01:30:01.79866Z",
     "iopub.status.idle": "2025-05-17T01:30:01.803899Z",
     "shell.execute_reply": "2025-05-17T01:30:01.803135Z",
     "shell.execute_reply.started": "2025-05-17T01:30:01.798952Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def shift_tokens_right(input_ids: torch.Tensor, pad_token_id: int, decoder_start_token_id: int):\n",
    "    \"\"\"\n",
    "    Shift input ids one token to the right.\n",
    "    \"\"\"\n",
    "    shifted_input_ids = input_ids.new_zeros(input_ids.shape)\n",
    "    shifted_input_ids[:, 1:] = input_ids[:, :-1].clone()\n",
    "    shifted_input_ids[:, 0] = decoder_start_token_id\n",
    "\n",
    "    # replace possible -100 values in labels by pad_token_id\n",
    "    shifted_input_ids.masked_fill_(shifted_input_ids == -100, pad_token_id)\n",
    "\n",
    "    return shifted_input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-17T01:30:19.607291Z",
     "iopub.status.busy": "2025-05-17T01:30:19.606617Z",
     "iopub.status.idle": "2025-05-17T01:30:19.75018Z",
     "shell.execute_reply": "2025-05-17T01:30:19.74938Z",
     "shell.execute_reply.started": "2025-05-17T01:30:19.607267Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"/kaggle/working/whisper-small-vi\",  # change to a repo name of your choice\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=2,  # increase by 2x for every 2x decrease in batch size\n",
    "    learning_rate=1e-5,\n",
    "    num_train_epochs = 2,\n",
    "    warmup_steps=500,\n",
    "    gradient_checkpointing=True,\n",
    "    fp16=True, \n",
    "    per_device_eval_batch_size=4,\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=225,\n",
    "    save_steps=1000,\n",
    "    eval_steps=1000,\n",
    "    logging_steps=25,\n",
    "    report_to=[\"tensorboard\"],\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"wer\",\n",
    "    greater_is_better=False,\n",
    "    push_to_hub=True,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    ")\n",
    "\n",
    "        \n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=vlsp_dict[\"train\"],\n",
    "    eval_dataset=vlsp_dict[\"val\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Test forward pass\n",
    "train_dataloader = trainer.get_train_dataloader()\n",
    "batch = next(iter(train_dataloader))\n",
    "\n",
    "print(\"\\nBatch keys:\", batch.keys())\n",
    "for k, v in batch.items():\n",
    "    if isinstance(v, torch.Tensor):\n",
    "        print(f\"{k} shape:\", v.shape)\n",
    "        print(f\"{k} dtype:\", v.dtype)\n",
    "\n",
    "# Prepare decoder input ids\n",
    "decoder_input_ids = shift_tokens_right(\n",
    "    batch[\"labels\"],\n",
    "    model.config.pad_token_id,\n",
    "    model.config.decoder_start_token_id\n",
    ")\n",
    "\n",
    "# Try forward pass\n",
    "test_outputs = model.forward(\n",
    "    input_features=batch[\"input_features\"],\n",
    "    labels=batch[\"labels\"],\n",
    "    decoder_input_ids=decoder_input_ids,\n",
    "    return_dict=True\n",
    ")\n",
    "\n",
    "print(\"\\nTest forward pass loss:\", test_outputs.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(batch.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-17T01:30:27.905849Z",
     "iopub.status.busy": "2025-05-17T01:30:27.905503Z",
     "iopub.status.idle": "2025-05-17T01:30:28.281457Z",
     "shell.execute_reply": "2025-05-17T01:30:28.280726Z",
     "shell.execute_reply.started": "2025-05-17T01:30:27.90582Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "processor.save_pretrained(training_args.output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-17T01:30:30.773108Z",
     "iopub.status.busy": "2025-05-17T01:30:30.772803Z",
     "iopub.status.idle": "2025-05-17T07:29:10.197855Z",
     "shell.execute_reply": "2025-05-17T07:29:10.195713Z",
     "shell.execute_reply.started": "2025-05-17T01:30:30.773087Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-17T07:43:45.34452Z",
     "iopub.status.busy": "2025-05-17T07:43:45.343655Z",
     "iopub.status.idle": "2025-05-17T07:45:43.475312Z",
     "shell.execute_reply": "2025-05-17T07:45:43.474728Z",
     "shell.execute_reply.started": "2025-05-17T07:43:45.34449Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "kwargs = {\n",
    "    \"dataset\": \"VLSP 10000\",  # a 'pretty' name for the training dataset\n",
    "    \"language\": \"vi\",\n",
    "    \"model_name\": \"Whisper Medium Vi - ASR\",  # a 'pretty' name for your model\n",
    "    \"finetuned_from\": \"openai/whisper-medium\",\n",
    "    \"tasks\": \"automatic-speech-recognition\",\n",
    "}\n",
    "\n",
    "trainer.push_to_hub(**kwargs)\n",
    "trainer.push_to_hub(**kwargs)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 7335883,
     "sourceId": 11833979,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31012,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
